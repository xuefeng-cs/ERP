data:
  name: jarod0411/zinc10M
lm_name: gptneo
lm:
  hidden_size: 64 # 128 as in chemgpt 4.7M
  intermediate_size: null # 4*hidden_size if null
  num_attention_heads: 16
  num_hidden_layers: 24
  attention_dropout: 0.1 # 0.0 as in chemgpt 4.7M
  embed_dropout: 0.1 # 0.0 as in chemgpt 4.7M
  resid_dropout: 0.1 # 0.0 as in chemgpt 4.7M
  summary_first_dropout: 0.1 # 0.1 as in chemgpt 4.7M
  max_length: null
  vocab_size: null
  eos_token_id: null
  max_position_embeddings: null
trainer:
  # output_dir: null
  evaluation_strategy: epoch
  # eval_steps: 2000 # ineffectual when evaluation_strategy!=steps
  per_device_train_batch_size: 256
  per_device_eval_batch_size: 256
  gradient_accumulation_steps: 1
  learning_rate: 1e-3
  weight_decay: 0.01
  max_grad_norm: 1.0
  num_train_epochs: 128
  lr_scheduler_type: cosine_with_restarts
  warmup_ratio: 0.05
  log_level: info
  logging_strategy: steps
  logging_steps: 1000 # ineffectual when logging_strategy!=steps
  save_strategy: epoch
  # save_steps: 2000 # ineffectual when save_strategy!=steps
  save_total_limit: 16
  use_cpu: false
  seed: 42
  bf16: false
  fp16: false
  tf32: false
  dataloader_num_workers: 1
  label_names: ["input_ids"] # nota bene : strangly seeming necessary to evaluate
  load_best_model_at_end: true
  metric_for_best_model: eval_loss
  greater_is_better: false
  deepspeed: null
  optim: adamw_torch
  report_to: tensorboard
  gradient_checkpointing: false
  torch_compile: true
trainercall:
  resume_from_checkpoint: null
